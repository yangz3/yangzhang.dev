<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.7/css/all.min.css">
    <base href="https://yangzhang.dev/">
    <title>Yang Zhang | Sensing research at CMU</title>
    <style>
        body {
            margin-top: 20px;
            font-family: sans-serif;
            font-weight: lighter;

        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #DB522F;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }

        h3 {
            font-size: 1.2em;
            color: #000000;
        }

        h4 {
            font-size: 1em;
            font-family: sans-serif;
            font-weight: lighter;
            color: #000000;
            margin-top: 10px;
            margin-bottom: 30px;
        }

        .strong {
            color: #DB522F;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }

        .annotation {
            margin-top: -0.5em;
            margin-bottom: 0.5em;
            font-size: 12px;
            line-height: 12px;
        }

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }

        div.line-of-research {
            background-color: #F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling: touch;
        }

        .fun-projects img {
            max-width: 100%;
        }

        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }

        hr.dash {
            border-top: 1px dashed #bbbbbb;
            margin-bottom: 15px;
            margin-top: 15px;
        }

        .switch {
            position: relative;
            display: block;
            width: 50px;
            height: 22px;
            float: left;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 3px;
            right: 0;
            bottom: -3px;
            left: 0;
            background-color: #ccc;
            transition: 0.5s;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 5px;
            bottom: 3px;
            background-color: white;
            transition: 0.5s;
        }

        .active .slider {
            background-color: #555;
        }

        .active .slider:before {
            transform: translateX(25px);
        }
    </style>
    <link rel="apple-touch-icon" sizes="180x180" href="//moon.feitsui.com/img/zhangyang/180x180.png">
    <link rel="icon" sizes="192x192" href="//moon.feitsui.com/img/zhangyang/192x192.png" type="image/png">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-48610112-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-48610112-3');
    </script>

</head>

<body>
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="/">Yang Zhang</a>
        </h1>
        <ul class="list-inline float-md-right social-icons">
            <li class="list-inline-item"><a href="/CV_YangZhang.pdf">CV/Resume</a></li>
            <li class="list-inline-item"><a href="https://scholar.google.com/citations?user=N6543cwAAAAJ">Google
                    Scholar</a></li>
            <li class="list-inline-item"><a href="https://github.com/yangz3">Github</a></li>
            <li class="list-inline-item"><a href="https://twitter.com/yanghci">Twitter</a></li>
            <li class="list-inline-item"><a href="mailto:yang.zhang@cs.cmu.edu">Email</a></li>
        </ul>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-8 col-lg-9 col-md-8">
                <p>
                    Yang Zhang is a 5th (final) year Ph.D. candidate at Human-Computer Interaction Institute (HCII), School of Computer Science, Carnegie
                    Mellon University, advised by Prof. <a href="http://www.chrisharrison.net/">Chris Harrison</a>. He
                    is also a Qualcomm Innovation Fellow. 
                </p>
                <p>
					He develops sensing techniques for next-generation human-computer interfaces that bring computing resources to users instead of forcing users to go to computing resources. Specifically, he builds sensors to enable natural interactions on mobile devices beyond touchscreens. He also creates sensors to detect user activities for personal and environment informatics which can lead to healthier and more efficient lives. 
                </p>
				
                <p>
					He publishes at <a href="https://dl.acm.org/event.cfm?id=RE151">CHI</a> (ACM CHI Conference on Human Factors in Computing Systems) and <a href="https://dl.acm.org/event.cfm?id=RE172">UIST</a> (ACM Symposium on User Interface Software and Technology), and has received 5 Best Paper (1%) and Honorable Mention Awards (5%).
                </p>
                <p>
                    Taxonomies of his completed research can be found below. More exciting projects are on the way.
                </p>
            </div>
            <div class="offset-xl-1 col-lg-3 col-md-4">
                <img src="/image/me_portrait.jpg" class="portrait">
            </div>
        </div>

        <div class="row taxonomy">
            <div class="col-lg-6 mb-3">
                <img src="/image/Taxonomy_YZ1.png">
            </div>
            <div class="col-lg-6 mb-3">
                <img src="/image/Taxonomy_YZ2.png">
            </div>
			<div class="col-lg-6 mb-3"><p class="annotation">[Research focus diagram inspired by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>]</p></div>
        </div>

        <div class="row">
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>
                    Research
                    <div class="float-right">
                        <small class="ml-2">Sort by Date</small>
                        <div class="switch sort-by-date">
                            <span class="slider"></span>
                        </div>
                    </div>
                </h2>
                <div class="research-projects">
                <div class="alert alert-secondary">
                    <h3>Wide-Area Sensors for Ubiquitous Sensing</h3>
                    <div>
                        Smart environment relies on robust and accurate sensing techniques. Closest to this vision are
						cameras and general-purpose sensor tags, and yet several key challenges remain (e.g., high cost, 
						demanding maintenance, low sensing versatility and resolution, visually unappealing, and privacy invasive). 
						To tackle these challenges, I build wide-area sensors that can sense at distance and monitor activities at 
						room-scale, enabling ubiquitous sensing with a sparse sensor deployment.
                    </div>
                </div>
				
                <div class="row research-project" data-sort="2019-10-01">
                    <div class="col-md-4">
                       <video loop muted playsinline poster="/image/toappear.png">
                       </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Sozu: Self-Powered Radio Tags for Building-Scale Activity Sensing
                        </h6>
                        <p class="text-muted">
                            Y Zhang, Y Iravantchi, H Jin, S Kumar and C Harrison (to appear at UIST 2019)
							[PDF upon requests]
                        </p>
                        
                        <p>
                            Sozu a low-cost sensing system that can detect a wide range of events wirelessly, through walls and without 
							line of sight, at whole-building scale. Instead of using batteries, Sozu tags convert energy 
							from activities that they sense into RF broadcasts, acting like miniature self-powered radio stations.
                        </p>
                    </div>
                </div>

                <div class="row research-project" data-sort="2018-10-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Vibrosight/thumbnail.jpg">
                            <source type="video/webm" src="/research/Vibrosight/thumbnail.webm">
                            <source type="video/mp4" src="/research/Vibrosight/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Vibrosight: Long-Range Vibrometry for Smart Environment Sensing
                        </h6>
                        <p class="text-muted">
                            Y Zhang, G Laput and C Harrison (UIST 2018)
							<a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                            <a class="info" href="https://github.com/FIGLAB/vibrosight">[Code]</a>
                        </p>
                        <p class="strong">
                            <i class="fas fa-medal"></i> Honorable Mention Award
                        </p>
                        <p>
                            Vibrosight senses activities across entire rooms using long-range laser vibrometry. Unlike
                            a microphone, our approach can sense physical vibrations at one specific point, making it
                            robust to interference from other activities and noisy environments. This property enables
                            detection of simultaneous activities, which has proven challenging in prior work.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project" data-sort="2018-05-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Wall/thumbnail.jpg">
                            <source type="video/webm" src="/research/Wall/thumbnail.webm">
                            <source type="video/mp4" src="/research/Wall/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Wall++: Room-Scale Interactive and Context-Aware Sensing
                        </h6>
                        <p class="text-muted">
                            Y Zhang, C Yang, S E. Hudson, C Harrison and A Sample (CHI 2018)
							<a class="info" href="https://youtu.be/175LB2OiMHs">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3173847">[DOI]</a>
                            <a class="info" href="/research/Wall/Wall.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <i class="fas fa-trophy"></i> Best Paper Award
                        </p>
                        <p>
                            Wall++ is a low-cost sensing approach that allows walls to become a smart infrastructure.
                            Our wall treatment and sensing hardware can track users' touch and gestures, as well as
                            estimate body pose if they are close. By capturing airborne electromagnetic noise, we can
                            also detect what appliances are active and where they are located.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project" data-sort="2017-05-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/SyntheticSensor/thumbnail.png">
                            <source type="video/webm" src="/research/SyntheticSensor/thumbnail.webm">
                            <source type="video/mp4" src="/research/SyntheticSensor/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Synthetic Sensors: Towards General-Purpose Sensing
                        </h6>
                        <p class="text-muted">
                            G Laput, Y Zhang and C Harrison (CHI 2017)
							<a class="info" href="https://youtu.be/aqbKrrru2co">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3025773">[DOI]</a>
                            <a class="info" href="/research/SyntheticSensor/SyntheticSensor.pdf">[PDF]</a>
                        </p>

                        <p>
                            In this work, we explore the notion of general-purpose sensing, wherein a single,
                            highly capable sensor can indirectly monitor a large context, without direct
                            instrumentation of objects. Further, through what we call Synthetic Sensors,
                            we can virtualize raw sensor data into actionable feeds, whilst simultaneously
                            mitigating immediate privacy issues.
                        </p>
                    </div>
                </div>

                <hr class="dash">
                <div class="alert alert-secondary">
                    <h3>Electric Field Tomography for Interactivity</h3>
                    <div>
                        Since my first project with Electric Field Tomography (Tomo 2015), I have been improving this
						technique as well as broadening its use cases. Electric Field Tomography features many advantages 
						of capacitive sensing but is of higher spatial resolution. The following projects demonstrate its 
                        applications in touch tracking and on-body sensing.
                    </div>
                </div>

                <div class="row research-project" data-sort="2017-05-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Electrick/thumbnail.png">
                            <source type="video/webm" src="/research/Electrick/thumbnail.webm">
                            <source type="video/mp4" src="/research/Electrick/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Electrick: Low-Cost Touch Sensing Using Electric Field Tomography
                        </h6>
                        <p class="text-muted">
                            Y Zhang, G Laput and C Harrison (CHI 2017)
							<a class="info" href="https://youtu.be/38h4-5FDdV4">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3025842">[DOI]</a>
                            <a class="info" href="/research/Electrick/Electrick.pdf">[PDF]</a>
                        </p>
                        <p>
                            Electrick is a low-cost and versatile sensing technique that enables touch input
                            on a wide variety of objects and surfaces, whether small or large, flat or irregular.
                            This is achieved by using electric field tomography in concert with an electrically
                            conductive material, which can be easily and cheaply added to objects and surfaces.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project" data-sort="2015-10-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Tomo/thumbnail.png">
                            <source type="video/webm" src="/research/Tomo/thumbnail.webm">
                            <source type="video/mp4" src="/research/Tomo/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Tomo: Wearable, Low-cost, Electrical Impedance Tomography for Hand Gesture Recognition
                        </h6>
                        <p class="text-muted">
                            Y Zhang and C Harrison (UIST 2015)
							<a class="info" href="https://youtu.be/N9c4hINa2Bk">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=2807480">[DOI]</a>
                            <a class="info" href="/research/Tomo/Tomo.pdf">[PDF]</a>
                        </p>
                        <p>
                            Tomo recovers the interior impedance geometry of a user's arm by measuring
                            the cross-sectional impedances from surface electrodes resting on the skin.
                            We integrated the technology into a prototype wristband, which can classify
                            gestures in real-time. Our approach is sufficiently compact and low-powered
                            that we envision this technique being integrated into future smartwatches to
                            allow hand gestures to work together with touchscreens.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project" data-sort="2016-10-01">
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Tomo2/thumbnail.png">
                            <source type="video/webm" src="/research/Tomo2/thumbnail.webm">
                            <source type="video/mp4" src="/research/Tomo2/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography
                        </h6>
                        <p class="text-muted">
                            Y Zhang, R Xiao and C Harrison (UIST 2016)
							<a class="info" href="https://youtu.be/6a8q7HON4_c">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=2984574">[DOI]</a>
                            <a class="info" href="/research/Tomo2/Tomo2.pdf">[PDF]</a>
                        </p>
                        <p>
                            We improved our prior work on wearable Electrical Impedance Tomography with
                            higher sampling speed and resolution. In turn, this enables superior interior
                            reconstruction and gesture recognition. More importantly, we use our new system
                            as a vehicle for experimentation -- we compare two EIT sensing methods and three
                            different electrode resolutions.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2018-05-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Pulp/thumbnail.jpg">
                            <source type="video/webm" src="/research/Pulp/thumbnail.webm">
                            <source type="video/mp4" src="/research/Pulp/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Pulp Nonfiction: Low-Cost Touch Tracking for Paper
                        </h6>
                        <p class="text-muted">
                            Y Zhang and C Harrison (CHI 2018)
							<a class="info" href="https://youtu.be/Y1Q0QCPdZys">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3173691">[DOI]</a>
                            <a class="info" href="/research/Pulp/Pulp.pdf">[PDF]</a>
                        </p>
                        <p>
                            We developed a sensing technique for paper to track finger input and also drawn input with
                            writing implements. Importantly, for paper to still be considered paper, our method had to
                            be very low cost. This necessitated research into materials, fabrication methods and
                            sensing techniques. We describe the outcome of our investigations and show that our method
                            can be sufficiently low-cost and accurate to enable new interactive opportunities with this
                            pervasive and venerable material.
                        </p>
                    </div>
                </div>

                <hr class="dash">
                <div class="alert alert-secondary">
                    <h3>Enhancing Smart Devices for Interactions beyond Touchscreens </h3>
					<div>
						Smart Devices can be only as smart as what they can sense. I build sensors to extend the sensing range 
						of these devices to see user postures, hand proximity, finger touch around the device. These sensing 
						capabilities serve as a secondary input channel besides touchscreens, which allows these smart devices 
                        to morph their UIs, fitting user context and being easier to use.
                    </div>
                </div>
				
                <div class="row research-project">
				<!-- 2019-10-01 -->

                    <div class="col-md-4">
                       <video loop muted playsinline poster="/image/toappear.png">
                       </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            ActiTouch: Robust Touch Detection for On-Skin AR/VR Interfaces
                        </h6>
                        <p class="text-muted">
                            Y Zhang, W Kienzle, Y Ma, S S. Ng, H Benko, C Harrison (to appear at UIST 2019)
							[PDF upon requests]
                        </p>
                        
                        <p>
                            ActiTouch is a new electrical method that enables precise on-skin touch segmentation by using the body as 
							an RF wave-guide. We combine this method with computer vision, enabling a system with both high tracking 
							precision and robust touch detection. Our system can enable touchscreen-like interactions on the skin.
                        </p>
                    </div>
                </div>
				
                <div class="row research-project">
				<!-- 2019-05-01 -->

                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Interferi/thumbnail.png">
                            <source type="video/webm" src="/research/Interferi/thumbnail.webm">
                            <source type="video/mp4" src="/research/Interferi/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Interferi: Gesture Sensing using On-Body Acoustic Interferometry
                        </h6>
                        <p class="text-muted">
                            Y Iravantchi, Y Zhang, E Bernitsas, M Goel, and C Harrison. (CHI 2019)
							<a class="info" href="https://youtu.be/_nMauMXDqf8">[Video]</a> 
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3300506">[DOI]</a>
                            <a class="info" href="/research/Interferi/Interferi.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <i class="fas fa-medal"></i> Honorable Mention Award
                        </p>
                        <p>
							Interferi is an on-body gesture sensing technique using acoustic interferometry. 
							We use ultrasonic transducers resting on the skin to create acoustic interference 
							patterns inside the wearerâ€™s body, which interact with anatomical features in complex, 
							yet characteristic ways. We focus on two areas of the body with great expressive power: 
							the hands and face. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2018-05-01 -->

                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/PostureAwareTablet/thumbnail.png">
                            <source type="video/webm" src="/research/PostureAwareTablet/thumbnail.webm">
                            <source type="video/mp4" src="/research/PostureAwareTablet/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Sensing Posture-Aware Pen+Touch Interaction on Tablets
                        </h6>
                        <p class="text-muted">
                            Y Zhang, M Pahud, C Holz, H Xia, G Laput, M McGuffin, X Tu, A Mittereder, F Su, W Buxton
                            and K Hinckley (CHI 2019)
							<a class="info" href="https://youtu.be/b8zE0BcGiZ0">[Video]</a> 
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3300285">[DOI]</a>
                            <a class="info" href="/research/PostureAwareTablet/PostureAwareTablet.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <i class="fas fa-medal"></i> Honorable Mention Award
                        </p>
                        <p>
                            The mobility of tablets affords interaction from various user-centric postures including
                            shifting hand grips, varying screen angle and orientation, planting the palm while writing
                            or sketching. We propose Posture-Aware Interface which morphs to a suitable frame of
                            reference, at the right time, and for the right (or left) hand.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2016-05-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/SkinTrack/thumbnail.png">
                            <source type="video/webm" src="/research/SkinTrack/thumbnail.webm">
                            <source type="video/mp4" src="/research/SkinTrack/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            SkinTrack:
                            Using the Body as an Electrical Waveguide for Continuous Finger Tracking on the Skin
                        </h6>
                        <p class="text-muted">
                            Y Zhang, J Zhou, G Laput and C Harrison (CHI 2016)
							<a class="info" href="https://youtu.be/9hu8MNuvCHE">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=2858082">[DOI]</a>
                            <a class="info" href="/research/SkinTrack/SkinTrack.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <i class="fas fa-medal"></i> Honorable Mention Award
                        </p>
                        <p>
                            SkinTrack is a wearable system that enables continuous touch tracking on the skin. It
                            consists of a signal-emitting ring and a sensing wristband with multiple electrodes. Due to
                            the phase delay inherent in a high-frequency AC signal propagating through the body, a
                            phase difference can be observed between pairs of electrodes, which we use to compute a 2D
                            finger touch coordinate.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2016-10-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/AuraSense/thumbnail.png">
                            <source type="video/webm" src="/research/AuraSense/thumbnail.webm">
                            <source type="video/mp4" src="/research/AuraSense/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing
                        </h6>
                        <p class="text-muted">
                            J Zhou, Y Zhang, G Laput and C Harrison (UIST 2016)
							<a class="info" href="https://youtu.be/gZGqkpuwzrA">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=2984568">[DOI]</a>
                            <a class="info" href="/research/AuraSense/AuraSense.pdf">[PDF]</a>
                        </p>
                        <p>
                            AuraSense enhances smartwatches with Electric Field Sensing to support multiple interaction
                            modalities. We identified four electrode configurations that can support six well-known
                            modalities of particular interest and utility, including gestures above the watchface and
                            touchscreen-like finger tracking on the skin.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2017-10-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/Pyro/thumbnail.png">
                            <source type="video/webm" src="/research/Pyro/thumbnail.webm">
                            <source type="video/mp4" src="/research/Pyro/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing
                        </h6>
                        <p class="text-muted">
                            J Gong, Y Zhang, X Zhou, XD Yang (UIST 2017)
							<a class="info" href="https://youtu.be/ww_2W787KFg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3126615">[DOI]</a>
                            <a class="info" href="/research/Pyro/Pyro.pdf">[PDF]</a>
                        </p>
                        <p>
                            Pyro is a micro thumb-tip gesture recognition technique based on thermal infrared signals
                            radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it
                            suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we
                            developed a self-contained prototype consisting of the infrared pyroelectric sensor, a
                            custom sensing circuit, and software for signal processing and machine earning.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2017-05-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/EMPhone/thumbnail.png">
                            <source type="video/webm" src="/research/EMPhone/thumbnail.webm">
                            <source type="video/mp4" src="/research/EMPhone/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances
                        </h6>
                        <p class="text-muted">
                            R Xiao, G Laput, Y Zhang and C Harrison (CHI 2017)
							<a class="info" href="https://youtu.be/eInfzdZ-9fE">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3025828">[DOI]</a>
                            <a class="info" href="/research/EMPhone/EMPhone.pdf">[PDF]</a>
                        </p>
                        <p>
                            We propose an approach where users simply tap a smartphone to an appliance to discover
                            and rapidly utilize contextual functionality. To achieve this, our prototype smartphone
                            recognizes physical contact with uninstrumented appliances through EMI sensing, and summons
                            appliance-specific interfaces.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
				<!-- 2018-10-01 -->
				
                    <div class="col-md-4">
                        <video loop muted playsinline poster="/research/LumiWatch/thumbnail.jpg">
                            <source type="video/webm" src="/research/LumiWatch/thumbnail.webm">
                            <source type="video/mp4" src="/research/LumiWatch/thumbnail.mp4">
                        </video>
                    </div>
                    <div class="col-md-8">
                        <h6>
                            LumiWatch: On-Arm Projected Graphics and Touch Input
                        </h6>
                        <p class="text-muted">
                            R Xiao, T Cao, N Guo, J Zhuo, Y Zhang and C Harrison (CHI 2018)
							<a class="info" href="https://youtu.be/VJNMrulWJ3k">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3173669">[DOI]</a>
                            <a class="info" href="/research/LumiWatch/LumiWatch.pdf">[PDF]</a>
                        </p>
                        <p>
                            LumiWatch is the first, fully-functional and self-contained projection smartwatch
                            implementation, containing the requisite compute, power, projection and touch-sensing
                            capabilities. Our watch offers more than five times that of a typical smartwatch display.
                            We demonstrate continuous 2D finger tracking with interactive, rectified graphics,
                            transforming the arm into a touchscreen.
                        </p>
                    </div>
                </div>

                <hr class="dash">

            </div>
            </div>
            <!-- /left column -->
            <!-- right column -->
            <div class="col-lg-4 mb-2">
                <h2>Latest News</h2>
                <ul class="news" style="font-size: 13px">
					<li>Aug 5 Working with Gregory Abowd to complete my thesis.</li>
					<li>Jun 21 Two papers got conditionally accepted at UIST 2019.</li>
					<li>May 21 Proposed my dissertation.</li>
					<li>Mar 25 10 days before UIST deadline. Start writing Thesis proposal.</li>
					<li>Mar 15 Two papers got CHI Honorable Mention Award!</li>
                    <li>2019 - Feb 25 UIST 2019 projects in full swing.</li>
                    <li>Dec 15 Thesis committee finalized. Will propose May 2019.</li>
                    <li>Nov 25 Zhuoshu joined Google Pittsburgh!</li>
                    <li>Oct 15 Attend UIST at Berlin.</li>
                    <li>Jun 6 Start internship at MSR, Redmond.</li>
                    <li>Apr 20 Attend CHI at Montreal.</li>
                    <li>2018 - Jan 19 New semester started. </li>
                    <li>Oct 22 Attended UIST 2017 at Quebec City.</li>
                    <li>Sep 23 One week off. Visiting Zhuoshu in New York.</li>
                    <li>
                        Sep 10 Invited to give a talk at <a href="http://tv.cctv.com/2017/09/10/VIDEa4ppB0d5YiO3mTNd1ial170910.shtml">CCTV2</a>.
                    </li>
                    <li>Jul 11 Disney projects in full swing.</li>
                    <li>Jul 5 Kayak and swim at North Shore.</li>
                    <li>May 8 Present and demo Electrick at CHI 2017.</li>
                    <li>2017 - Jan 2 Back in Pittsburgh.</li>
                </ul>
                <ul class="news" id="news-more" style="font-size: 13px">
                    <li>Dec 22 Visit <a href="http://www.a-su.com.cn/">ASU</a> and Ling.</li>
                    <li>Dec 17 Visit Hong Kong to see my wife.</li>
                    <li>Oct 20 Back in Pittsburgh.</li>
                    <li>Oct 16 Attend UIST 2016 @ Tokyo, give AuraSense presentation.</li>
                    <li>Oct 09 Talk about research and share experience living abroad with <a href="http://dsd.future-lab.cn/">ICMLL
                            lab</a>.</li>
                    <li>Oct 06 Wonderful wedding ceremony with two families and friends at Beijing.</li>
                    <li>Sep 22 post-CHI party at Union Grill. Preparing for my wedding.</li>
                    <li>Aug 28 CHI 2017 projects final push.</li>
                    <li>Jun 26 Three lab papers got accepted at UIST 2016. Go FIGlab!</li>
                    <li>Jun 1 Summer projects for CHI 2017 are in full swing.</li>
                    <li>May 16 I got married!</li>
                    <li>Apr 13 UIST 2016 Paper submitted. Fly to St. Louis for weekends.</li>
                    <li>Apr 1 UIST 2016 in full swing. </li>
                    <li>Mar 23 Qualcomm Innovation Fellowship finalist presentation and demo. </li>
                    <li>Jan 26 Pittsburgh Penguins vs. New Jersey Devils. We won! </li>
                    <li>2016 - Jan 9 Got back to Pittsburgh. New semester started! </li>
                    <li>Dec 16 Filming for CHI project done. Flying back to Beijing.</li>
                    <li>Nov 30 CHI rebuttals submitted.</li>
                    <li>Nov 26 Host friends from high school over thanksgiving.</li>
                    <li>Nov 11 Reunion dinner with CoDelab friends. Wonderful UIST2015.</li>
                    <li>Nov 7 Heading for UIST 2015, Charlotte, NC. </li>
                    <li>Oct 28 Demo at Engedget, NYC. </li>
                    <li>Oct 26 Received a happy birthday suprise from the lab. </li>
                    <li>Sep 12 CHI 2016 projects final push. </li>
                    <li>Aug 31 First day as a PhD student. </li>
                    <li>Aug 23 Summer project user study began. </li>
                    <li>Aug 20 Tomo and Quantifying Electrostatic Haptic Feedback got accepted by UIST and ITS
                        2015. </li>
                    <li>Jun 3 Summer projects in full swing. </li>
                    <li>May 20 Tour at DC with family. </li>
                    <li>Apr 15 Party after UIST submission at Butter Joint. </li>
                    <li>Apr 7 UIST 2015 in full swing. </li>
                    <li>Mar 24 Make food storage in the lab for UIST late night work. </li>
                    <li>Mar 21 Had a wonderful visit at Cornell Tech NYC and Ithaca. </li>
                    <li>Feb 14 Extreme cold weather in St Louis. </li>
                    <li>Jan 21 User test for the Fitts Law Project. 10 down, 10 to go! </li>
                    <li>Jan 13 Back at Pittsburgh. </li>
                    <li>2015 - Jan 5 V1.0 bio-impedance meter board is sent for printing. </li>
                    <li>Dec 14 Went back home. Happy birthday mom! </li>
                    <li>Oct 9 ACM UIST conference Student Innovation Contest 1st Most Creative Award for our
                        project!</li>
                    <li>Aug 22 Finished my internship at Kinoma, Marvell. </li>
                    <li>Jun 18 Won the first place in IoT Hackathon, evironment category. </li>
                    <li>May 29 The third day as intern. Developed an alarm using openweathermap and Google TTS API.
                    </li>
                    <li>May 25 Arrive at Santa Clara for the summer intern. </li>
                    <li>May 07 Final exam of 15213, done with high score. </li>
                    <li>Apr 23 Final presentation of ZipperSense. </li>
                    <li>Apr 18 Travel to Phily, play basketball with friends. </li>
                    <li>Apr 04 V3.0 PCB board for the ZipperSense is sent for printing. </li>
                    <li>Mar 07 V2.0 PCB board for the ZipperSense is sent for printing. </li>
                    <li>Feb 21 V1.0 PCB board for the final project of gadget class is being printed. </li>
                    <li>Jan 22 Turned an old cushion and a box into a stray cat's nest.</li>
                    <li>Jan 20 30-minutes running, 1st day. The goal is to beats the number of days last semester.</li>
                    <li>Jan 13 New semester begins. I'm so excited.</li>
                    <li>Jan 11 Back to Pittsburgh.</li>
                    <li>Jan 05 St Louis snow storm. Store food and water.</li>
                    <li>2014 - Jan 01 Went to St Louis to spend the first day of the new year with my girl friend.</li>


                    <li>Dec 27 Went to The Grand Canyon which is truly grand.</li>
                    <li>Dec 24 Went to Las Vegas.</li>
                    <li>Dec 02 Successful presentation of 24780 C++ class's final project--"Interactive Fish"</li>
                    <li>Nov 28 Have a big Thanks-giving Turkey dinner in Jake's parents' house.</li>
                    <li>Nov 14 Paper and video finished for TEI. Everyday 1000-yards-swimming over 66 days!</li>
                    <li>Nov 05 Prototype1 of Heart Pulse is completed. Busy finishing the work for TEI conference.</li>
                    <li>Oct 21 Three final projects proposal. I'm really excited to get start.</li>
                    <li>Oct 9 Top-5 program in C++ class. Reward is bowling bowl game tickets!</li>
                    <li>Oct 6 Everyday 1000-yards-swimming, over 28 days.</li>
                    <li>Oct 1 Perform well in C++ python midterm. Finally make our silicon mask work with
                        solenoids.</li>
                    <li>Sep 21 Everyday 1000-yards-swimming, over 13 days.</li>
                    <li>Sep 9 Visit Prof. Dale's house. Having fun.</li>
                    <li>Aug 30 First week in CMU. Cool! Now going to St. Louis for weekend</li>
                    <li>Aug 13 Orientation as new graduated student. End with a hugh BBQ!</li>
                    <li>Aug 8 Arrive at Pittsburg.</li>
                    <li>Jul 28 Bought one pair of hiking shoes.</li>
                    <li>Jul 21 Pack up stuff for studing abroad.</li>
                    <li>Jul 20 Send our pet dog to school.</li>
                    <li>Jul 19 Change domain name to "bennyzhang.me"</li>
                    <li>Jul 7 Return to Beijing.</li>
                    <li>Jul 2 Arrive at Liaoyuan.</li>
                    <li>Jun 27 Best barbecue I ever had. Also had the local spicy soup and noodle. Super spicy!</li>
                    <li>Jun 25 Depart from my dorm. Head for Changchun, my roommate's hometown.</li>
                    <li>2013 - Jun 15 Pack up my stuff. Only ten days to leave my Beihang University.</li>
                </ul>
                <a id="toggle-more-news" href="#">More &gt</a>

                <div class="mt-3 tweets">
                    <a class="twitter-timeline" width="100%" height="800" href="https://twitter.com/yanghci">
                        Tweets by yanghci
                    </a>
                    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                </div>
            </div>
            <!-- /right column -->
        </div>

        <h2>Fun Projects</h2>
        <div class="row fun-projects">
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://github.com/yangz3/DOTA2-Player-Survival-Rate-Analysis">
                <img src="/project/dota2/thumbnail.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="/project/mickey.png">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="/project/bulbasaur/thumbnail.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="/project/weddinggift/thumbnail.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="/project/ring/1.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="/project/colormesh/thumbnail.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/5Fge4-N5Avo">
                <img src="/project/others/metal.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/H70mu4YS5oo">
                <img src="/project/others/wikiknow.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://vimeo.com/87513578">
                <img src="/project/others/jump.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://vimeo.com/110517949">
                <img src="/project/others/plantie.png"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://vimeo.com/76547312">
                <img src="/project/others/face.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/I_74lUWW4XY">
                <img src="/project/others/chair.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://vimeo.com/104072436">
                <img src="/project/others/twitterlight.jpg"></a>
            </div>
        </div>
    </div>
</body>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.3/dist/jquery.min.js"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').toggle();
        $('#news-more').is(':visible') ? $(this).text('< Hide') : $(this).text('More >');
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = $(this)[0];
            var rect = video.getBoundingClientRect();

            if (
                rect.top >= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });

    var researchProjects = $('.research-projects').html();

    var researchProjectsSorted = $('div.research-project').sort(function (a, b) {
        var contentA = $(a).attr('data-sort');
        var contentB = $(b).attr('data-sort');
        return (contentA < contentB) ? -1 : (contentA > contentB) ? 1 : 0;
    });

    $('.sort-by-date').click(function () {
        $(this).toggleClass('active');
        if ($(this).hasClass('active')) {
            $('.research-projects').html(researchProjectsSorted);
        } else {
            $('.research-projects').html(researchProjects);
        }
    });
</script>

</html>